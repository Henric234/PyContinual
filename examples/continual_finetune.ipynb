{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Simple Example of Continual Finetune\n",
        "\n",
        "This notebook's purpose is to demonstrate the implementation of the soft-masking concept (refer to the [TSS](https://arxiv.org/abs/2310.09436)). It is not designed to yield effective results in real-world scenarios. Its simplicity lies in the fact that:\n",
        "\n",
        "*   We avoid using advanced packages, including huggingface.\n",
        "*   We employ a basic fully connected network instead of any pre-trained language models or LSTM.\n",
        "*   The data is synthetic, and we do not implement a real tokenizer or task-specific loss\n"
      ],
      "metadata": {
        "id": "a19g2_rSwf_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary packages"
      ],
      "metadata": {
        "id": "hb0zBYMSyjHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random, os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import math\n",
        "\n"
      ],
      "metadata": {
        "id": "PoOsMF3RyiWy"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct a basic tokenizer. This tokenizer's vocabulary is created from the provided corpus. It is not suitable for real-world applications, as this simplistic approach cannot manage any words that are not already in the corpus."
      ],
      "metadata": {
        "id": "62Kr5Z6J516h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(corpus):\n",
        "  # Build vocabulary\n",
        "\n",
        "  vocab = defaultdict(int)\n",
        "  idx = 1 # 0 is used as padding token id\n",
        "  for text in corpus:\n",
        "      for word in text.split():\n",
        "        if word not in vocab:\n",
        "          vocab[word] = idx\n",
        "          idx += 1\n",
        "\n",
        "  # Use vocabulary\n",
        "  tokenizerd_corpus = []\n",
        "  for text in corpus:\n",
        "      tokenized_text = []\n",
        "      for word in text.split():\n",
        "          tokenized_text.append(vocab[word])\n",
        "      tokenizerd_corpus.append(tokenized_text)\n",
        "\n",
        "  return {'idx': tokenizerd_corpus}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TMujK8vjwlH5"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we implement a helper function to assist in tokenizing each instance in the dataset."
      ],
      "metadata": {
        "id": "p_WDgzkj6ewJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_pad(examples, max_length):\n",
        "\n",
        "    result = {}\n",
        "    new_example = []\n",
        "    for example in examples['idx']:\n",
        "      if max_length < len(example): # trancate\n",
        "        new_example.append(example[:max_length])\n",
        "      else:\n",
        "        difference = max_length - len(example)\n",
        "        new_example.append(example + [0] * difference)\n",
        "\n",
        "    result['idx'] = new_example\n",
        "\n",
        "    #Lets also give some synthetic label here for pre-training task\n",
        "    label_ids = [0,1]\n",
        "    result['labels'] = []\n",
        "    for idx in result['idx']:\n",
        "      result['labels'].append(random.sample(label_ids, 1))\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "lpWwAAdf6ieA"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to create a custom PyTorch dataset, since our data is formatted as a dictionary."
      ],
      "metadata": {
        "id": "_qlKSXzj6yjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['idx'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        data_tensor = {}\n",
        "        for key, value in self.data.items():\n",
        "          data_item = self.data[key][idx]\n",
        "          data_tensor[key] = torch.tensor(data_item, dtype=torch.float)\n",
        "\n",
        "        return data_tensor\n"
      ],
      "metadata": {
        "id": "aSlUYZO06y3p"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is inspired by [SupSup](https://github.com/RAIVNLab/supsup/blob/master/mnist.ipynb). We overwrite the ``nn.linear`` function so that the network training is transformed into training for popup scores (see [TSS](https://arxiv.org/abs/2310.09436)).\n",
        "\n"
      ],
      "metadata": {
        "id": "BXDZbyiy7B7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_compute_mask_impt(model, compute_impt):\n",
        "    for n, m in model.named_modules():\n",
        "        if isinstance(m, NNSubnetworkSoftmask):\n",
        "            m.compute_mask_impt = compute_impt\n",
        "\n",
        "def set_ft_task(model, ft_task):\n",
        "    for n, m in model.named_modules():\n",
        "        if isinstance(m, NNSubnetworkSoftmask):\n",
        "            m.ft_task = ft_task\n",
        "\n",
        "# Subnetwork forward from hidden networks\n",
        "class GetSubnet(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, scores):\n",
        "        return (scores >= 0).float() # Use 0 as threshold. this is related to the signed_constant initialization\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        # Send the gradient g straight-through on the backward pass. so that it is trainable\n",
        "        return g\n",
        "\n",
        "class NNSubnetworkSoftmask(nn.Linear):\n",
        "    def __init__(self, *args, num_tasks=1, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.num_tasks = num_tasks\n",
        "        self.scores = nn.ParameterList(\n",
        "            [\n",
        "                nn.Parameter(self.mask_init())\n",
        "                for _ in range(num_tasks)\n",
        "            ]\n",
        "        )\n",
        "        self.impt_mask =nn.ParameterList(\n",
        "            [\n",
        "                nn.Parameter(torch.zeros(self.weight.size())).requires_grad_(False)\n",
        "                for _ in range(self.num_tasks)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Alphas are used later when we compute the importance of the scores.\n",
        "        self.alphas =nn.Parameter(torch.ones(self.weight.size()))\n",
        "\n",
        "        # Keep weights untrained\n",
        "        self.weight.requires_grad = False\n",
        "        self.signed_constant()\n",
        "\n",
        "\n",
        "    def copy_score(self, ft_task):\n",
        "        with torch.no_grad():\n",
        "            self.scores[ft_task+1].copy_(self.scores[ft_task].clone())\n",
        "\n",
        "    def mask_init(self):\n",
        "        scores = torch.Tensor(self.weight.size())\n",
        "        nn.init.kaiming_uniform_(scores, a=math.sqrt(5))\n",
        "        return scores\n",
        "\n",
        "    def signed_constant(self):\n",
        "        fan = nn.init._calculate_correct_fan(self.weight, 'fan_in')\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        std = gain / math.sqrt(fan)\n",
        "        self.weight.data = self.weight.data.sign() * std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.compute_mask_impt:  # Whether it is to compute the importance\n",
        "            selected_mask = self.scores[self.ft_task]\n",
        "\n",
        "            subnet = GetSubnet.apply(selected_mask)\n",
        "            w = self.weight * subnet * self.alphas\n",
        "            x = F.linear(x, w, self.bias)\n",
        "\n",
        "        else:\n",
        "            selected_mask = self.scores[self.ft_task]\n",
        "            subnet = GetSubnet.apply(selected_mask)\n",
        "            w = self.weight * subnet\n",
        "            x = F.linear(x, w, self.bias)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"NNSubnetworkSoftmask({self.weight.size(0)}, {self.weight.size(1)})\"\n",
        "\n",
        "class NNSoftmask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NNSoftmask, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(300, 50)\n",
        "        self.fc1 = NNSubnetworkSoftmask(50,30, num_tasks=2, bias=False)\n",
        "        self.fc2 = NNSubnetworkSoftmask(30,10, num_tasks=2, bias=False)\n",
        "        self.head = nn.Linear(10,1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.word_embeddings.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.word_embeddings(x)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.sigmoid(self.head(x).mean(1))\n",
        "        return x\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "hGCuQYPF7CRy"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can initialize our synthetic data and the model."
      ],
      "metadata": {
        "id": "Ob7ktF5S79sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "        '''\n",
        "        Apparently Prides Osteria had a rough summer as evidenced by the almost empty dining room at 6:30 on a Friday night. However new blood in the kitchen seems to have revitalized the food from other customers recent visits. Waitstaff was warm but unobtrusive. By 8 pm or so when we left the bar was full and the dining room was much more lively than it had been. Perhaps Beverly residents prefer a later seating. After reading the mixed reviews of late I was a little tentative over our choice but luckily there was nothing to worry about in the food department. We started with the fried dough, burrata and prosciutto which were all lovely. Then although they don't offer half portions of pasta we each ordered the entree size and split them. We chose the tagliatelle bolognese and a four cheese filled pasta in a creamy sauce with bacon, asparagus and grana frita. Both were very good. We split a secondi which was the special Berkshire pork secreto, which was described as a pork skirt steak with garlic potato purée and romanesco broccoli (incorrectly described as a romanesco sauce). Some tables received bread before the meal but for some reason we did not. Management also seems capable for when the tenants in the apartment above began playing basketball she intervened and also comped the tables a dessert. We ordered the apple dumpling with gelato and it was also quite tasty. Portions are not huge which I particularly like because I prefer to order courses. If you are someone who orders just a meal you may leave hungry depending on you appetite. Dining room was mostly younger crowd while the bar was definitely the over 40 set. Would recommend that the naysayers return to see the improvement although I personally don't know the former glory to be able to compare. Easy access to downtown Salem without the crowds on this month of October.\n",
        "        ''',\n",
        "        '''\n",
        "        The food is always great here. The service from both the manager as well as the staff is super. Only draw back of this restaurant is it's super loud. If you can, snag a patio table!\n",
        "        ''',\n",
        "        '''\n",
        "        This place used to be a cool, chill place. Now its a bunch of neanderthal bouncers hopped up on steroids acting like the can do whatever they want. There are so many better places in davis square where they are glad you are visiting their business. Sad that the burren is now the worst place in davis.\n",
        "        '''\n",
        "        ]\n",
        "\n",
        "\n",
        "tokenizerd_text = tokenizer(corpus)\n",
        "max_length = 30\n",
        "truncate_pad_tokenized_text = truncate_pad(tokenizerd_text,max_length)\n",
        "\n",
        "my_dataset = CustomDataset(truncate_pad_tokenized_text)\n",
        "batch_size = 2\n",
        "data_loader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "subnetworrk_softmask = NNSoftmask()\n"
      ],
      "metadata": {
        "id": "UKns-MzU8Gzi"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the first task, we do not need to apply any masking. We train the network (i.e., the scores) in a conventional manner."
      ],
      "metadata": {
        "id": "4mV53TaU8SZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_task = 0\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(subnetworrk_softmask.parameters(), lr=0.003)\n",
        "set_compute_mask_impt(subnetworrk_softmask,False)\n",
        "set_ft_task(subnetworrk_softmask,ft_task)\n",
        "\n",
        "epochs = 10\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  i = 0\n",
        "  for step, batch in enumerate(data_loader):\n",
        "    i += 1\n",
        "    if i % 100 == 0:\n",
        "        print(f'Training loss at step {i}: {running_loss/(i*batch_size)}')\n",
        "    input_ids = batch['idx'].long()\n",
        "    labels = batch['labels']\n",
        "\n",
        "    outputs = subnetworrk_softmask(input_ids)\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    if e < 1 and step < 1:\n",
        "      for n, p in subnetworrk_softmask.named_parameters():\n",
        "        if p.grad is not None:\n",
        "            print(f'Gradient of param \"{n}\" with size {tuple(p.size())} detected')\n",
        "\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "\n",
        "    print(f'Training loss: {running_loss / (len(data_loader) * batch_size)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQG0dep78bnp",
        "outputId": "4c2baf8f-6ee5-4ca7-bf74-4a55d977880e"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of param \"fc1.scores.0\" with size (30, 50) detected\n",
            "Gradient of param \"fc2.scores.0\" with size (10, 30) detected\n",
            "Gradient of param \"head.weight\" with size (1, 10) detected\n",
            "Gradient of param \"head.bias\" with size (1,) detected\n",
            "Training loss: 0.12747827172279358\n",
            "Training loss: 0.25286777317523956\n",
            "Training loss: 0.11871679127216339\n",
            "Training loss: 0.2254461646080017\n",
            "Training loss: 0.10435687005519867\n",
            "Training loss: 0.21180221438407898\n",
            "Training loss: 0.09677727520465851\n",
            "Training loss: 0.19502855837345123\n",
            "Training loss: 0.09435197710990906\n",
            "Training loss: 0.1859283596277237\n",
            "Training loss: 0.0890825018286705\n",
            "Training loss: 0.17168926447629929\n",
            "Training loss: 0.08337169885635376\n",
            "Training loss: 0.1547119840979576\n",
            "Training loss: 0.08222860842943192\n",
            "Training loss: 0.1536419317126274\n",
            "Training loss: 0.0745910257101059\n",
            "Training loss: 0.1509472206234932\n",
            "Training loss: 0.06745259463787079\n",
            "Training loss: 0.1325201839208603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fine-tuning the first task (``ft_task=0``), we need to calculate the importance of the scores in each layer. This calculation is based on cross-entropy. Once determined using the gradient, we then normalize the importance. Additionally, we copy the trained scores to the next task as an initialization step, allowing knowledge transfer to the subsequent task."
      ],
      "metadata": {
        "id": "W9WFNRzp8hzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_task = 0\n",
        "set_compute_mask_impt(subnetworrk_softmask, True)\n",
        "set_ft_task(subnetworrk_softmask,ft_task)\n",
        "\n",
        "tss_impt_dict = {}\n",
        "\n",
        "for step, batch in enumerate(data_loader):\n",
        "  input_ids = batch['idx'].long()\n",
        "  labels = batch['labels']\n",
        "\n",
        "  outputs = subnetworrk_softmask(input_ids)\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "\n",
        "  for n, m in subnetworrk_softmask.named_modules():\n",
        "      if isinstance(m, NNSubnetworkSoftmask):\n",
        "          if n in tss_impt_dict:\n",
        "              tss_impt_dict[n] += m.alphas.grad.clone().detach()\n",
        "          else:\n",
        "              tss_impt_dict[n] = m.alphas.grad.clone().detach()\n",
        "\n",
        "\n",
        "subnetworrk_softmask.zero_grad() # Remove gradients\n",
        "\n",
        "# Normalize the importance\n",
        "def impt_norm(impt):\n",
        "    tanh = torch.nn.Tanh()\n",
        "    for layer in range(impt.size(0)):\n",
        "        impt[layer] = (impt[layer] - impt[layer].mean()) / impt[\n",
        "            layer].std()  # 2D, we need to deal with this for each layer\n",
        "    impt = tanh(impt).abs()\n",
        "\n",
        "    return impt\n",
        "\n",
        "for n, m in subnetworrk_softmask.named_modules():\n",
        "    if isinstance(m, NNSubnetworkSoftmask):\n",
        "        with torch.no_grad():\n",
        "            m.impt_mask[ft_task][impt_norm(tss_impt_dict[n]) >= 0.5] = 1\n",
        "            print(f'Name and usage: {n}, {(m.impt_mask[ft_task].sum() / m.impt_mask[ft_task].numel()).item()}') # importance mask\n",
        "\n",
        "# Copy the scores\n",
        "for n, m in subnetworrk_softmask.named_modules():\n",
        "    if isinstance(m, NNSubnetworkSoftmask):\n",
        "        m.copy_score(ft_task)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nzB4gMp8kOE",
        "outputId": "aafcefcb-5145-40b7-8dda-e2c55737cae4"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name and usage: fc1, 0.6053333282470703\n",
            "Name and usage: fc2, 0.5266666412353516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can begin training the second task (``ft_task=2``). For simplicity, we use the same data as in the first task. During training, we apply soft-masking to the gradients of the scores to preserve previous knowledge. After training the second task, we need to compute the importance as in the previous steps (not shown in the code)."
      ],
      "metadata": {
        "id": "FKZH4hOY4JmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_task = 1\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(subnetworrk_softmask.parameters(), lr=0.003)\n",
        "set_compute_mask_impt(subnetworrk_softmask,False)\n",
        "set_ft_task(subnetworrk_softmask,ft_task)\n",
        "\n",
        "epochs = 10\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  i = 0\n",
        "  for step, batch in enumerate(data_loader):\n",
        "    i += 1\n",
        "    if i % 100 == 0:\n",
        "        print(f'Training loss at step {i}: {running_loss/(i*batch_size)}')\n",
        "    input_ids = batch['idx'].long()\n",
        "    labels = batch['labels']\n",
        "\n",
        "    outputs = subnetworrk_softmask(input_ids)\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    if e < 1 and step < 1:\n",
        "      for n, p in subnetworrk_softmask.named_parameters():\n",
        "        if p.grad is not None:\n",
        "            print(f'Gradient of param \"{n}\" with size {tuple(p.size())} detected')\n",
        "\n",
        "    for n, m in subnetworrk_softmask.named_modules():\n",
        "        if isinstance(m, NNSubnetworkSoftmask):\n",
        "            m.scores[ft_task].grad *= (1-tss_impt_dict[n])\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    print(f'Training loss: {running_loss / (len(data_loader) * batch_size)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVJw1Qv44J0X",
        "outputId": "e1510294-c21b-4167-c6f8-414bad7b6b94"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of param \"fc1.scores.1\" with size (30, 50) detected\n",
            "Gradient of param \"fc2.scores.1\" with size (10, 30) detected\n",
            "Gradient of param \"head.weight\" with size (1, 10) detected\n",
            "Gradient of param \"head.bias\" with size (1,) detected\n",
            "Training loss: 0.06010271608829498\n",
            "Training loss: 0.12154484912753105\n",
            "Training loss: 0.051853425800800323\n",
            "Training loss: 0.10218348354101181\n",
            "Training loss: 0.045971132814884186\n",
            "Training loss: 0.09850922599434853\n",
            "Training loss: 0.04364640638232231\n",
            "Training loss: 0.08559411019086838\n",
            "Training loss: 0.03648237884044647\n",
            "Training loss: 0.08484786003828049\n",
            "Training loss: 0.04133094474673271\n",
            "Training loss: 0.07143541425466537\n",
            "Training loss: 0.031948160380125046\n",
            "Training loss: 0.06391788274049759\n",
            "Training loss: 0.02935045398771763\n",
            "Training loss: 0.06814335472881794\n",
            "Training loss: 0.028552938252687454\n",
            "Training loss: 0.058869631960988045\n",
            "Training loss: 0.027650360018014908\n",
            "Training loss: 0.049211161211133\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}